{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/24 06:56:49 WARN Utils: Your hostname, user-System-Product-Name resolves to a loopback address: 127.0.1.1; using 114.213.214.100 instead (on interface enp36s0f1)\n",
      "24/12/24 06:56:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/24 06:56:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/12/24 06:56:51 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/12/24 06:56:51 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "24/12/24 06:56:51 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,count,mean,udf,sum,when\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Typhoon Analyze\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "spark.conf.set(\"spark.rapids.sql.enable\",\"true\")\n",
    "\n",
    "data = spark.read.option(\"header\", True).csv(\"../design/data/risk_assessment.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-----+---+----+--------+---------+--------------------+--------------------------------+\n",
      "|storm_id|year|month|day|hour|latitude|longitude|               grade|Indicator of landfall or passage|\n",
      "+--------+----+-----+---+----+--------+---------+--------------------+--------------------------------+\n",
      "|    5101|1951|    2| 19|   6|    20.0|    138.5| Tropical Depression|                               0|\n",
      "|    5101|1951|    2| 19|  12|    20.0|    138.5| Tropical Depression|                               0|\n",
      "|    5101|1951|    2| 19|  18|    23.0|    142.1| Tropical Depression|                               0|\n",
      "|    5101|1951|    2| 20|   0|    25.0|    146.0|Tropical Cyclone ...|                               0|\n",
      "|    5101|1951|    2| 20|   6|    27.6|    150.6|Tropical Cyclone ...|                               0|\n",
      "+--------+----+-----+---+----+--------+---------+--------------------+--------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+\n",
      "|year|typhoon_count|\n",
      "+----+-------------+\n",
      "|1951|          792|\n",
      "|1952|          836|\n",
      "|1953|          864|\n",
      "|1954|          727|\n",
      "|1955|          858|\n",
      "+----+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----+------+-------------+\n",
      "|year|season|typhoon_count|\n",
      "+----+------+-------------+\n",
      "|1951|  Fall|          273|\n",
      "|1951|Spring|          184|\n",
      "|1951|Summer|          239|\n",
      "|1951|Winter|           96|\n",
      "|1952|  Fall|          417|\n",
      "+----+------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 提取台风与时间的趋势\n",
    "#columns year typhoon_count\n",
    "typhoon_trend = data.groupBy(\"year\").agg(count(\"storm_id\").alias(\"typhoon_count\")).orderBy(\"year\")\n",
    "typhoon_trend.show(5)\n",
    "\n",
    "# 提取台风与季节的趋势\n",
    "#columns season typhoon_count\n",
    "season_trend = data.withColumn(\"season\", when(col(\"month\").isin([\"12\", \"1\", \"2\"]), \"Winter\")\n",
    "                                      .when(col(\"month\").isin([\"3\", \"4\", \"5\"]), \"Spring\")\n",
    "                                      .when(col(\"month\").isin([\"6\", \"7\", \"8\"]), \"Summer\")\n",
    "                                      .when(col(\"month\").isin([\"9\", \"10\", \"11\"]), \"Fall\")\n",
    "                                      .otherwise(\"Unknown\")) \\\n",
    "                   .groupBy(\"year\", \"season\").agg(count(\"storm_id\").alias(\"typhoon_count\")).orderBy(\"year\", \"season\")\n",
    "season_trend.show(5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-------------+\n",
      "|storm_id|year|      positon|\n",
      "+--------+----+-------------+\n",
      "|    9114|1991|[34.4, 137.7]|\n",
      "|    9117|1991|[32.7, 129.7]|\n",
      "|    9119|1991|[32.8, 129.7]|\n",
      "|    9209|1992|[32.7, 133.1]|\n",
      "|    9210|1992|[32.7, 130.5]|\n",
      "|    9211|1992|[32.5, 131.9]|\n",
      "|    9304|1993|[33.7, 134.7]|\n",
      "|    9305|1993|[31.2, 131.0]|\n",
      "|    9306|1993|[32.5, 129.7]|\n",
      "|    9311|1993|[42.8, 144.2]|\n",
      "|    9313|1993|[30.9, 130.3]|\n",
      "|    9314|1993|[33.5, 135.3]|\n",
      "|    9407|1994|[32.6, 132.8]|\n",
      "|    9411|1994|[41.0, 140.2]|\n",
      "|    9426|1994|[33.5, 135.3]|\n",
      "|    9514|1995|[31.8, 130.2]|\n",
      "|    9606|1996|[30.4, 130.5]|\n",
      "|    9609|1996|[24.2, 123.7]|\n",
      "|    9612|1996|[26.3, 127.9]|\n",
      "|    9621|1996|[24.4, 125.4]|\n",
      "+--------+----+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first, lit, concat_ws,udf\n",
    "\n",
    "# 提取台风与区域的趋势\n",
    "# columns positon typoon_landed\n",
    "region_trend = data.filter(col(\"Indicator of landfall or passage\") == \"1\")\n",
    "\n",
    "def to_point(la,lo):\n",
    "        return [la,lo]\n",
    "\n",
    "to_point_udf=udf(to_point)\n",
    "region_trend = region_trend.withColumn('positon',to_point_udf(col('latitude'),col('longitude')))\n",
    "\n",
    "region_trend = region_trend.groupBy('storm_id', 'year').agg(\n",
    "    first(\"positon\").alias(\"positon\"),\n",
    ").orderBy(\"year\")\n",
    "\n",
    "region_trend.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|year|            landings|\n",
      "+----+--------------------+\n",
      "|1991|[[34.4, 137.7], [...|\n",
      "|1992|[[32.7, 133.1], [...|\n",
      "|1993|[[33.7, 134.7], [...|\n",
      "|1994|[[32.6, 132.8], [...|\n",
      "|1995|     [[31.8, 130.2]]|\n",
      "+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_list\n",
    "\n",
    "# 将每年的 positon 合并成一个列表\n",
    "region_trend_grouped = region_trend.groupBy(\"year\") \\\n",
    "                                   .agg(collect_list(\"positon\").alias(\"landings\"))\n",
    "\n",
    "region_trend_grouped.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|year|            landings|\n",
      "+----+--------------------+\n",
      "|1991|[34.4, 137.7],[32...|\n",
      "|1992|[32.7, 133.1],[32...|\n",
      "|1993|[33.7, 134.7],[31...|\n",
      "|1994|[32.6, 132.8],[41...|\n",
      "|1995|       [31.8, 130.2]|\n",
      "+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "# Convert the array column to a string\n",
    "region_trend_grouped = region_trend_grouped.withColumn(\"landings\", concat_ws(\",\", \"landings\"))\n",
    "region_trend_grouped.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "season_trend.coalesce(1).write.mode(\"overwrite\").option(\"header\",True).csv(\"result/llmdata/year_season\")\n",
    "region_trend_grouped.coalesce(1).write.mode(\"overwrite\").option(\"header\",True).csv(\"result/llmdata/landings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
